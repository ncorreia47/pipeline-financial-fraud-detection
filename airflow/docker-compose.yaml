x-airflow-common:
  &airflow-common
  image: ${AIRFLOW_IMAGE}
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USERNAME}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DATABASE}
    AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    _PIP_ADDITIONAL_REQUIREMENTS: "pandas boto3 kaggle kagglehub dbt-core dbt-postgres"
  volumes:
    - ./dags/dbt_dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./kaggle:/home/airflow/.config/kaggle
    - ./datasets:/opt/airflow/datasets
  depends_on:
    - postgres

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USERNAME}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DATABASE}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  airflow-webserver:
    <<: *airflow-common
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler

  airflow-init:
    <<: *airflow-common
    command: >
      bash -c "airflow db upgrade && \
      airflow users create \
      --username admin \
      --password admin \
      --firstname FIRST_NAME \
      --lastname LAST_NAME \
      --role Admin \
      --email admin@example.org"
  
    dbt:
      image: dbt-labs/dbt:latest
      container_name: dbt-container
      volumes:
        - ./dbt_data_pipeline:/dbt
        - ./dbt_data_pipeline/target:/dbt/target
      working_dir: /dbt
      environment:
        DBT_PROFILE_DIR: /dbt/config 
      command: run
      networks:
        - default
      extra_hosts:
      - "host.docker.internal:host-gateway"


volumes:
  postgres-db-volume: